{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shajad121/Credit-card-fefault-prediction/blob/main/credit_card_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Credit card default prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uyc48vib961E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project is about the Credit card default prediction analysis. When we start this project, it is very difficult to understand and look very complex.\n",
        "In first step we upload the data and then we start to find the duplicate values and nulls values and missing values etc. but in this data there is no missing values and duplicate values. After completion of 1st step in 2nd step we plot the data in form of graph, pie plot and bar plot and line plot and also box plot, and in box plot we find some outlier but these outliers are not affect the result because they are very less in number. And we also fine some hidden information by plot the graph like there are 34% male are defaulter but 21% women are defaulter it means that the womens are less defaulter as compare to male. And also from graduate school defaulter that is 19% then university comes with 24% defaulter then high school comes with 25% defaulters,according to this chart from age 60-70 the default percent 29% present in this group. from the age group 20-30 There are 7421 customers are not defaulters but 2197 customers are defaulters.\n",
        "\n",
        "fromm the age group 30-40 there are 9862 customers are not defaulters but 2276 customere are defaulters.\n",
        "\n",
        "from age group 40-50 there are 4979 customers are non defaulter but 1485 customere are defaukters.\n",
        "\n",
        "from agew group50-60 there are 1759 customere are non defaulters but 582 customers are defaulter.\n",
        "\n",
        "from the age group of 60-70 there are 225 customere are non deffault but 89 customere arre also defaulters.in SEX plot single customers are least defaulter witth 21% then married customers are comes with 23.5% defaulters After plot the data we plot the heatmap of the data and then we find the relationship between the feature and we did some feature engineering in the data. We remove the labels of the data because that labels are not readable and difficult to understand and we make the dataâ€™s 1st row as the labels of the data.\n",
        "After the feature engineering we apply some algorithm, 1) Logistic Regression, 2)Random forest ,3)XG Boost and we also use the dummy model  to describe the our model and make them easy to understand. And we also do some cross validation in every algorithm.\n",
        "After applying these algorithm we select the Random Forest as a best algorithm that gives the best score. It gives the precision 0.668 and recall 0.364 and F1 score 0.471 percent. After selecting the algorithm we do some feature engineering and do some cross validation but there is not much change in the result. Logistic Regression model has the highest recall but the lowest precision, if the business cares recall the most, then this model is the best candidate. If the balance of recall and precision is the most important metric, then Random Forest is the ideal model. Since Random Forest has slightly lower recall but much higher precision than Logistic Regression, I would recommend Random Forest.    \n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Shajad121/Credit-card-fefault-prediction.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is aimed at the case of customers default payments in Taiwan. from the perspective of risk manegement the result of predictive accuracy of the estimated probabillity of default will be more valueable then the binary result of classification- credible or not credible client."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define your businese objective**"
      ],
      "metadata": {
        "id": "uCD6bMzWJchf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main objective is classified the customers in terms of is he/she credible or not credible customer. and minimise the risk of default credit card payments."
      ],
      "metadata": {
        "id": "KvwO5b4vJbYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline \n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "from pprint import pprint\n",
        "import joblib\n",
        "import imblearn\n",
        "import missingno as msno\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn import metrics  \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive \n",
        "from google.colab import drive\n",
        "drive.mount ('/content/drive/')\n",
        "# Load Dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/credit card prediction /default of credit card clients.xls')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.bar(df,figsize=(18,6))"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 30001 rowes and 25 columns present in this data.\n",
        "All the columns are object and the size of the dataset is 5.7+ mb \n",
        "There is no duplicate values present in this data as well as no null valuesa."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amount  :  Credit amount\n",
        "\n",
        "Gender  :  Gender\n",
        "\n",
        "Education  :  Educational qualification\n",
        "\n",
        "Marital status  :  Married or not\n",
        "\n",
        "Age  :  Age\n",
        "\n",
        "History of pass payments  :  Paymentts made in past\n",
        "\n",
        "Amount of bill statement  :  Bill amount\n",
        "\n",
        "Amount of previous payment  :  Previous amount paid"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df:\n",
        "  print(f'Unique values for {i}: {df[i].unique()}')\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creat a copy to avoide the data erission\n",
        "df2= df.copy()"
      ],
      "metadata": {
        "id": "Nxagmg4AMBZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the lable's rows\n",
        "df2.columns = [''] * len(df2.columns)"
      ],
      "metadata": {
        "id": "1l_CR0N_MBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(4)"
      ],
      "metadata": {
        "id": "m82bRrXbMBUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert the 1 row as header\n",
        "new_header = df2.iloc[0] #grab the first row for the header\n",
        "df2 = df2[1:] #take the data less the header row\n",
        "df2.columns = new_header "
      ],
      "metadata": {
        "id": "_alkzfDQMBRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.rename(columns={'PAY_0':'PAY_1'},inplace = True)\n",
        "df2.columns"
      ],
      "metadata": {
        "id": "YBI-Xl4MMBOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.replace([np.inf, -np.inf], np.nan)\n",
        "df2 = df2.dropna()\n",
        "df2 = df2.reset_index()"
      ],
      "metadata": {
        "id": "zfHENErkbfDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make the copy of our dataset to avoide the miss the data. we change the pay_0 tto pay_1."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# sex\n",
        "plt.figure(figsize=(15,6))\n",
        "ax=sns.countplot(x=df2['SEX'],hue='default payment next month', data=df2)\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "to calculate the % of the defaul according to sex"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this chart there are 9015 male has no default  but 2873 male  customers are defaulted.\n",
        "\n",
        "And  14349 female customers are not default but only 3763 female customera are defaulted.\n",
        "\n",
        "So it meanse female customers has less default then the male cusstomers. \n",
        "\n",
        "**There are 34% male are defaulter but 21% womens customers are defaulter so womens are more safest customere then mmens**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# education\n",
        "plt.figure(figsize=(15,6))\n",
        "ax=sns.countplot(x=df2['EDUCATION'],hue='default payment next month', data=df2)\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To count the customers has default or not according to there education qualification."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the Graduate shool There are 8549 customers are not defaulted but 2036 customers are defaulted.\n",
        "\n",
        "From the University there are 10700 customers are not defaulted but 3330 customers are defaulted.\n",
        "\n",
        "Fromm the High school there aree 3680 customers are not defaulted and 1237 customers are defaulted.\n",
        "\n",
        "and other we include them in outliers.\n",
        "\n",
        "**from graduate shool have least defaulter that is 19% then University comes with 24% defaulters then High school comes with 25% defaulters**\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.countplot(x=df2['MARRIAGE'],hue='default payment next month', data=df2)\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To count how much married and non married cusstomers are default or not"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the 1 there are 10453 married customers are not default and 3206 customers are defaulted.\n",
        "\n",
        "From the single there are 12623 single customers are not default but 334 single  customers are defaulted.\n",
        "\n",
        "**in this plot single customers are least defaulter witth 21% then married customers are comes with 23.5% defaulters**"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.countplot(x=df2['AGE'],hue='default payment next month', data=df2)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the default customers acccording to the age\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is not show cleare so we creaet the group of agge."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(15,6))\n",
        "ax=sns.countplot(x=df2['PAY_1'],hue='default payment next month', data=df2)\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the default 1st pay."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from -2(pay duly) there are 2394 customers are not default but 365 customera are default from -1(pay duly) there are 4732 customers are not default but 954 cusstomers are defaulted. from 0(not duly) ther are 12849 custommers are not defaulted but 1888 customers are defaulted. from1(payment delay 1 month) there are 2436 customers are not defaulted but 1252 customers are defaulted. from 2 (payments delay 2 month)there are 823 customere are not defaulted but 1844 customere are defaulted. from 3(payment delay 3 month ) there are 78 customers are not defaulted but 244 customers are defaulted.and so on....."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "col1 = ['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "list(enumerate(col1))"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.figure(figsize=(25,9))\n",
        "for i in enumerate(col1):\n",
        "  plt.subplot(2,3,i[0]+1)\n",
        "  sns.countplot(i[1],hue='default payment next month', data = df2)\n",
        "  plt.xticks(rotation =45)\n",
        "  for p in ax.patches:\n",
        "    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A6V-iCo5OaUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.countplot(x=df2['AGE'],hue='default payment next month', data=df2)"
      ],
      "metadata": {
        "id": "nZ2xUPc-PFwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the defaulty by age"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to  this chart \n",
        "\n",
        "from the age group 20-30 There are 7421 customers are not defaulters but 2197 customers are defaulters.\n",
        "\n",
        "fromm the age group 30-40 there are 9862 customers are not defaulters  but 2276 customere are defaulters.\n",
        "\n",
        "from age group 40-50 there are 4979 customers are non defaulter but 1485 customere are defaukters.\n",
        "\n",
        "from agew group50-60 there are 1759 customere are non defaulters but 582 customers are defaulter.\n",
        "\n",
        "from the age group of 60-70 there are 225 customere are non deffault but 89 customere arre also defaulters.\n",
        "\n",
        "**according to this chart from age 60-70 the default percent 29% present in this group**"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Use boxplot to identify any outlier in credit limit\n",
        "\n",
        "df2['LIMIT_BAL'].plot(kind=\"box\")\n",
        "plt.ylabel('No. of Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to calculate the outliers"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " There are some outlier present in limitt balance"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# 8 Use histogram to view the distribution of credit limit\n",
        "plt.hist(df2[\"LIMIT_BAL\"])\n",
        "plt.xlabel('Credit_Limit in NT$')\n",
        "plt.ylabel('NO. of Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to finde out the outlier"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to the plot there are some outlier present after the 50 % credit limit"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Create a new column \"HAS_DEF\" to indicate customers who have at least 1 deafult payment from PAY_1 to Pay_6\n",
        "# 0 : no default ; 1: has default\n",
        "\n",
        "def_condition = (df2.PAY_1 >1) | (df2.PAY_2 >1) | (df2.PAY_3 >1) | (df2.PAY_4 >1) | (df2.PAY_5 >1) | (df2.PAY_6 >1)\n",
        "df2.loc[def_condition, \"HAS_DEF\"] = 1\n",
        "df2.loc[df2.HAS_DEF.isna(), \"HAS_DEF\"] = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset a dataframe with the records that have default\n",
        "\n",
        "has_default = df2[df2['HAS_DEF']== 1]\n",
        "default_trend = has_default[['PAY_6','PAY_5','PAY_4','PAY_3','PAY_2','PAY_1']].sum(axis=0)\n",
        "\n",
        "# Draw a line chart to show the trend. The lower the number, the shorter delayed payment\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(default_trend)\n",
        "plt.xticks(['PAY_6','PAY_5','PAY_4','PAY_3','PAY_2','PAY_1'],['Apr','May','Jun','Jul','Aug','Sep'])\n",
        "\n",
        "plt.xlabel('Months in 2005',fontweight='bold')\n",
        "plt.ylabel('Total delayed months',fontweight='bold')\n",
        "plt.title('Delayed payment trend',fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P2-hw1ArPj0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are sharp up word trend between May to Aug and after Aug there is negetive dowwnword start"
      ],
      "metadata": {
        "id": "DBxHnb_bPvy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make 6 boxplots to visualize bill amounts and the total months of delayed payment\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Make \"PAY_1\" to\"PAY_6\" , 'BILL_AMT1\" to'BILL_AMT6\" into a list respectively\n",
        "pay_list = [ f\"PAY_{i}\" for i in range(1, 7) ]\n",
        "bill_amt_list = [ f\"BILL_AMT{i}\" for i in range(1, 7) ]\n",
        "\n",
        "fig, ax = plt.subplots(3,2, figsize=(20,9))\n",
        "\n",
        "for pay, bill_amt, myax in zip(pay_list, bill_amt_list, ax.flatten()):\n",
        "    \n",
        "    # Prepare data for boxploting\n",
        "    data = []\n",
        "    for i in sorted(has_default[pay].unique()):\n",
        "        temp = has_default.loc[has_default[pay] == i, bill_amt]\n",
        "        data.append(temp)\n",
        "        \n",
        "    # Make boxplot for each PAY variable\n",
        "    myax.boxplot(data, showfliers=False,) \n",
        "    myax.set_xticklabels(sorted(has_default[pay].unique()))\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xeNagqXVPjyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to identified the outliers"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no much outlier present in this data."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pair Plot visualization code\n",
        "plt.figure(figsize=(50,50))\n",
        "sns.pairplot(df2[:6],hue='default payment next month' ,data=df2 )"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis (1) :  Credit limit does not affect default likehood.\n",
        "\n",
        "Alternative hypothesis(1)  :  Credit limit impactt likehood.\n",
        "\n",
        "\n",
        "Null hypothesis(2)  :  Education does not affect default likehood.\n",
        "\n",
        "Alternative hypothesis(2)  :  Education impact default likehood.\n",
        "\n",
        "\n",
        "Null hypothesis(3)  :  Age dose not affect default likehood\n",
        "\n",
        "Alternative hypothesis(3)  :  Age impact default likehood"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis  :  Credit limit does not affect default likehood.\n",
        "\n",
        "Alternative hypothesis  :  Credit limit impactt likehood.\n",
        "\n",
        "Set significance level to 0.05."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "no_def_limit = df2.loc[df2['HAS_DEF']==0,'LIMIT_BAL']\n",
        "def_limit = df2.loc[df2['HAS_DEF']==1,'LIMIT_BAL']\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind_from_stats\n",
        "\n",
        "ttest_ind_from_stats(mean1=np.mean(no_def_limit),std1=np.std(no_def_limit),nobs1=len(no_def_limit),\n",
        "                     mean2=np.mean(def_limit),std2=np.std(def_limit),nobs2=len(def_limit))"
      ],
      "metadata": {
        "id": "NI040aXNT92-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use ttest_ind_from_stats metod to  obtain the p value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a p value as 0, therefore we need to reject the null hypothesis and accept the alternative hypothesis. Credit limit has an impact on payment default."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis  :  Education does not affect default likehood.\n",
        "\n",
        "Alternative hypothesis  :  Education impact default likehood.\n",
        "\n",
        "Set significance level to 0.05."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the contigence table of education and default data\n",
        "\n",
        "edu_def_contigence = pd.crosstab(df2['HAS_DEF'], df2['EDUCATION'], margins=False)"
      ],
      "metadata": {
        "id": "tNvTZkS3GcHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Use Chi-squared test to test if education affects default likelihood.\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "from scipy.stats import chi2_contingency\n",
        "stat, p, dof, expected = chi2_contingency(edu_def_contigence)\n",
        "\n",
        "p"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use chisquare and chi2_contingency staticaltest."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the p value is close to 0, we will reject the null hypothesis and accept the alternative hypothesis. Because education has a strong correlation with default probability, we should keep this variable in the machine learning model."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis  :  Age dose not affect default likehood\n",
        "\n",
        "Alternative hypothesis  :  Age impact default likehood\n",
        "\n",
        "Set significance level to 0.05."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "age_def_contigence = pd.crosstab(df2['HAS_DEF'], df2['AGE'], margins=False)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use chi-squared test to test if age affects default likelihood.\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "from scipy.stats import chi2_contingency\n",
        "stat, p, dof, expected = chi2_contingency(age_def_contigence)\n",
        "\n",
        "p"
      ],
      "metadata": {
        "id": "wiI1MwKVP-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use chisquare and chi2_contingency staticaltest."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p value is smaller than significance level \n",
        ", we will reject the null hypothesis and accept the alternative hypothesis, which is age has impact on default probability."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no sunch missing values ressent in this dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# we fine the some outliers in limit balance\n",
        "outliers  = df2.loc[df2['LIMIT_BAL']>900000]\n",
        "outliers"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In Education we notice that %,^bothe are recorded as unknown and 0 is also not define so we change alll of them into 0\n",
        "#Change values 4, 5, 6 to 0 and define 0 as 'others'\n",
        "#1=graduate school, 2=university, 3=high school, 0=others\n",
        "\n",
        "df2[\"EDUCATION\"] = df2[\"EDUCATION\"].replace({4:0,5:0,6:0})\n",
        "df2[\"EDUCATION\"].value_counts()"
      ],
      "metadata": {
        "id": "xt-ptgQTXSr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We change the  1 into male and 2 femail in sex columns anf we aolso merge the 4,5,6 ionto zero for better understanding"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Define predictor variables and target variable\n",
        "X = df2.drop(columns=['ID','default payment next month'])\n",
        "Y = df2['default payment next month'].astype('int')\n",
        "\n",
        "# Save all feature names as list\n",
        "feature_cols = X.columns.tolist() \n",
        "\n",
        "# Extract numerical columns and save as a list for rescaling\n",
        "X_num = X.drop(columns=['SEX', 'EDUCATION', 'MARRIAGE', 'AGE'])\n",
        "num_cols = X_num.columns.tolist()   "
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We drop the columns ID and default payment next month from the X variacle.\n",
        "and we only select default payment next year as Y variable."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the datta into train test \n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "T8m0HqOzdzFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use SMOTE to over sample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 42)\n",
        "sm.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "cDQAsWuZdytI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weuse 80% data as training data and 20% data as test data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "ecvnox6zRHsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rescale the data\n",
        "# split the datta into train test \n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "std.fit_transform(X_train)\n",
        "std.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "g17XM_pERSWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Check target classes balancec\n",
        "cla_bal = df2['default payment next month'].value_counts(normalize=True)\n",
        "print(cla_bal)\n",
        "\n",
        "# Plot the classes\n",
        "cla_bal.plot(kind = 'bar')\n",
        "plt.title('Nondefault(0) and default(1) comparison',fontweight = \"bold\")\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With typical default classification problems, we expect imbalanced classes as we know most people will not default. This dataset is also imbalanced, with 78% non-default vs. 22% default."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1   **Logistic regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "dN6HosgDiIoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the model\n",
        "Y_pred = lr.predict(X_test)\n",
        "Y_pred"
      ],
      "metadata": {
        "id": "WwqcVCUpiaAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the auc score\n",
        "scores = cross_val_score(lr, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_lr = np.mean(scores)\n",
        "roc_auc_lr"
      ],
      "metadata": {
        "id": "6NLLdi_hjION"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple logistic regression the roc_auc score is 0.64018 percent."
      ],
      "metadata": {
        "id": "Hau29az9SnUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Randomized search for the best C parameter\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Split data with SMOTE \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2,random_state=42) \n",
        "\n",
        "# Rescale data\n",
        "std = StandardScaler()\n",
        "std.fit_transform(X_train)\n",
        "std.fit_transform(X_test)\n",
        "\n",
        "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=42)\n",
        "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
        "clf = RandomizedSearchCV(logistic, distributions, random_state=42)\n",
        "\n",
        "lr_best= clf.fit(X_train, Y_train)   \n",
        "\n",
        "print(lr_best.best_params_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tuned model and parameters\n",
        "\n",
        "joblib.dump(lr_best,\"logreg_sm_tuned.pkl\") "
      ],
      "metadata": {
        "id": "7Dw209hPmhf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the best model\n",
        "lr_best = joblib.load(\"logreg_sm_tuned.pkl\")"
      ],
      "metadata": {
        "id": "w8eK5NxTmSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get ROC_AUC score of tuned model on training data\n",
        "\n",
        "scores_tuned = cross_val_score(lr_best, X_train, Y_train, scoring = \"roc_auc\", cv = 5)\n",
        "roc_auc_lr_best = np.mean(scores_tuned)\n",
        "\n",
        "print(f'ROC_AUC score after tuning parameters:{roc_auc_lr_best:.3f}')\n"
      ],
      "metadata": {
        "id": "YLbC-XuDmR6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save best ROC_AUC \n",
        "\n",
        "joblib.dump(roc_auc_lr_best,\"logreg_ROC_AUC_CV.pkl\") "
      ],
      "metadata": {
        "id": "DgNcyXdOiwDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_lr_best = joblib.load(\"logreg_ROC_AUC_CV.pkl\")\n",
        "roc_auc_lr_best"
      ],
      "metadata": {
        "id": "pzbzkzhMiv-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the fiting the crossvalidation the roc_auc score is 0.636 percent so we can says that our model is not much overfit."
      ],
      "metadata": {
        "id": "_TlMe-HgTAbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Model Evaluation**\n",
        "\n",
        "**Precision_Recall and F1 Score**"
      ],
      "metadata": {
        "id": "DxQ0TELkopSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute Precision, Recall and F1 score\n",
        "\n",
        "def get_pre_rec_f1(model,X_test,Y_test):\n",
        "    Y_pred = model.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "    \n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \n",
        "    print(f'Precision:{precision:.3f}\\nRecall:{recall:.3f}\\nF1 score:{F1:.3f}')"
      ],
      "metadata": {
        "id": "EgTyD9f6mR2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision and recall of Logistic Regression model\n",
        "\n",
        "print('Logistic Regression model on test data:')\n",
        "get_pre_rec_f1(lr_best, X_test, Y_test)"
      ],
      "metadata": {
        "id": "_8unCGsAna1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION MATRIX**"
      ],
      "metadata": {
        "id": "cKqdt_L4TOkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting the confusion matrix\n",
        "cf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "print(cf_matrix)\n"
      ],
      "metadata": {
        "id": "iuy7f_ZwRElh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 **Random Forest**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and fit the SMOTE for reduce imbalanc in dataset \n",
        "from imblearn.over_sampling import SMOTE\n",
        "fit_sample = SMOTE()\n",
        "fit_sample.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "6BJ6mBeCsL3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the Random Forest\n",
        "rdm = RandomForestClassifier()\n",
        "rdm.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "c-WdgqMSxWxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict model\n",
        "Y_pred = rdm.predict(X_test)\n",
        "Y_pred"
      ],
      "metadata": {
        "id": "kR3LUdTZxWpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use model's default parameters to get cross validation score\n",
        "scores = cross_val_score(rdm, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_rf = np.mean(scores)\n",
        "print(f'ROC_AUC score : {roc_auc_rf}')"
      ],
      "metadata": {
        "id": "oJsxW3odxWmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The roc_auc score of simple randomforest is 0.7667 percent."
      ],
      "metadata": {
        "id": "v0jfHaK2UXGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Create parameter grid  \n",
        "param_grid = {'max_depth': [60, 90, 110],\n",
        "              'min_samples_leaf': [3, 4, 5],\n",
        "              'min_samples_split': [8, 10, 12],\n",
        "              'n_estimators': [100, 200, 300]}\n",
        "\n",
        "# Instantiate the model\n",
        "clf_rf = RandomForestClassifier()\n",
        "\n",
        "# Instantiate grid search model\n",
        "grid_search = GridSearchCV(estimator = clf_rf, param_grid = param_grid,    \n",
        "                          cv = 3, n_jobs = -1, verbose = 1)\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "grid_search.best_params_\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tuned model\n",
        "\n",
        "joblib.dump(grid_search,\"RandomForest_tuned_final.pkl\")\n",
        "grid_search = joblib.load(\"RandomForest_tuned_final.pkl\")"
      ],
      "metadata": {
        "id": "jig0cHCqJu4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best parameters to fit the model\n",
        "\n",
        "rf_best = RandomForestClassifier(**grid_search.best_params_)   \n",
        "rf_best.fit(X_train,Y_train)\n",
        "\n",
        "scores_best = cross_val_score(rf_best, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_best = np.mean(scores_best)\n",
        "\n",
        "print(f'ROC_AUC training score after tuning for Random Forest: {roc_auc_best:.3f}')"
      ],
      "metadata": {
        "id": "jQHZKytfJwSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Model Evaluation**\n",
        "\n",
        "**Precision_Recall and F1 Score**"
      ],
      "metadata": {
        "id": "-CtlneWwJ7uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute precision, recall and F1 score of Random Forest model on test data\n",
        "\n",
        "print('Random Forest model on test data:')\n",
        "get_pre_rec_f1(rf_best, X_test, Y_test)"
      ],
      "metadata": {
        "id": "U0kdWiIPJwPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION MATRIC**"
      ],
      "metadata": {
        "id": "GUywYTmhTgDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "cf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "print(cf_matrix)\n"
      ],
      "metadata": {
        "id": "kbBjrw-3RwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model -3  **XG Boost**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "xg =XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg.fit(X_train,Y_train)\n",
        "# Predict on the model\n",
        "Y_pred = xg.predict(X_test)\n",
        "Y_pred"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(rdm, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_rf = np.mean(scores)\n",
        "print(f'ROC_AUC score : {roc_auc_rf}')"
      ],
      "metadata": {
        "id": "bOOHplK__MJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Instantiate models with initial best guess parameters\n",
        "\n",
        "params = { \n",
        "    'gamma':0,\n",
        "    'learning_rate':0.01, \n",
        "    'max_depth':3, \n",
        "    'colsample_bytree':0.6,\n",
        "    'subsample':0.8,\n",
        "    'scale_pos_weight':3.5,\n",
        "    'n_estimators':1000,\n",
        "    'objective':'binary:logistic', \n",
        "    'reg_alpha':0.3    \n",
        "}\n",
        "    \n",
        "# Instantiate model\n",
        "clf_xgb = XGBClassifier(**params)\n",
        "\n",
        "# Use model's initial parameters to get cross validation score\n",
        "scores = cross_val_score(clf_xgb, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_xgb = np.mean(scores)\n",
        "\n",
        "print(f'ROC_AUC training score with initial best parameters for XGBoost: {roc_auc_xgb:.3f}')\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees\n",
        "n_estimators = np.arange(200,1000,200)\n",
        "\n",
        "# Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
        "# The larger gamma is, the more conservative the algorithm will be\n",
        "gamma = np.arange(0.1,0.6,0.1)\n",
        "\n",
        "# Default 0.3, range(0,1)\n",
        "learning_rate = np.arange(0.1,0.6,0.1)\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = list(range(3,8,1))\n",
        "\n",
        "# Subsample ratio of the training instances.Range(0,1)\n",
        "subsample = np.arange(0.5,0.9,0.1)\n",
        "\n",
        "# Subsample ratio of columns when constructing each tree. Range(0,1)\n",
        "colsample_bytree = np.arange(0.5,0.9,0.1)\n",
        "\n",
        "# Control the balance of positive and negative weights\n",
        "# Sum(negative instances) / sum(positive instances)\n",
        "scale_pos_weight = [1,3.5]\n",
        "\n",
        "\n",
        "# Create the random grid\n",
        "random_grid_xgb = {'n_estimators': n_estimators,\n",
        "                   'gamma': gamma,\n",
        "                   'learning_rate':learning_rate,\n",
        "                   'max_depth': max_depth,\n",
        "                   'subsample':subsample,\n",
        "                   'colsample_bytree':colsample_bytree,\n",
        "                   'scale_pos_weight':scale_pos_weight\n",
        "                  }\n",
        "pprint(random_grid_xgb)"
      ],
      "metadata": {
        "id": "BiFdg6PtAany"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use randomized search to find best parameters\n",
        "\n",
        "xgboost = XGBClassifier()\n",
        "xgb_random = RandomizedSearchCV(estimator = xgboost, \n",
        "                                param_distributions = random_grid_xgb, \n",
        "                                n_iter = 100, \n",
        "                                cv = 3, \n",
        "                                verbose=1, \n",
        "                                random_state=42, \n",
        "                                n_jobs = -1,\n",
        "                                scoring ='roc_auc')\n",
        "\n",
        "\n",
        "xgb_random.fit(X_train, Y_train)   \n",
        "xgb_random.best_params_, xgb_random.best_score_\n",
        "\n",
        "print(xgb_random.best_params_,xgb_random.best_score_)"
      ],
      "metadata": {
        "id": "-Jhoqz5mAake"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tuned model for future use\n",
        "\n",
        "joblib.dump(xgb_random,'xgb_random_final.pkl')\n",
        "\n",
        "grid_search = joblib.load(\"xgb_random_final.pkl\")"
      ],
      "metadata": {
        "id": "2d3oGOlOAahJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best parameters to fit the model\n",
        "\n",
        "xg_best = XGBClassifier(**grid_search.best_params_)   \n",
        "xg_best.fit(X_train,Y_train)\n",
        "\n",
        "scores_best = cross_val_score(xg_best, X_train, Y_train, scoring =\"roc_auc\", cv = 5)\n",
        "roc_auc_best = np.mean(scores_best)\n",
        "\n",
        "print(f'ROC_AUC training score after tuning for Random Forest: {roc_auc_best:.3f}')"
      ],
      "metadata": {
        "id": "oaJNawUxPZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute precision, recall and F1 score of tuned XGBoost model on test data\n",
        "\n",
        "print('XGBoost model on test data:')\n",
        "\n",
        "get_pre_rec_f1(xgb_random, X_test, Y_test)"
      ],
      "metadata": {
        "id": "EbdZ6ietAadD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION MATRIC**"
      ],
      "metadata": {
        "id": "GP2rmmrgT0t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "cf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "id": "jbbF1YU7AaZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot all 3 confusion matrics** "
      ],
      "metadata": {
        "id": "u1UY00a1eD7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix of 3 models\n",
        "\n",
        "fig,ax=plt.subplots(1,3, figsize=(15,5))\n",
        "\n",
        "plot_confusion_matrix(lr_best, X_test, Y_test, ax = ax[0], values_format=\"d\")\n",
        "ax[0].set_title(\"Logistic Regression\")\n",
        "\n",
        "plot_confusion_matrix(rf_best, X_test, Y_test, ax = ax[1], values_format=\"d\")\n",
        "ax[1].set_title(\"Random Forest\")\n",
        "\n",
        "plot_confusion_matrix(xgb_random, X_test, Y_test, ax = ax[2], values_format=\"d\")\n",
        "ax[2].set_title(\"XGBoost\")"
      ],
      "metadata": {
        "id": "_vTFvuxScE2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot ROC_AUC curve of all models**"
      ],
      "metadata": {
        "id": "poRPHvngeROF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC_AUC curve of 3 models\n",
        " \n",
        "fig,ax=plt.subplots(figsize=(10,5))\n",
        "\n",
        "plot_roc_curve(lr_best, X_test, Y_test,ax=ax, color=\"blue\",label='Logistic Regression')\n",
        "plot_roc_curve(rf_best, X_test, Y_test,ax=ax, color=\"black\",label='Random Forest')\n",
        "plot_roc_curve(xgb_random, X_test, Y_test,ax=ax, color=\"red\",label='XGBoost')\n",
        "\n",
        "plt.title('ROC/AUC of 3 models')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "amx3bOEGcEzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Precusion_recall curves of all models**"
      ],
      "metadata": {
        "id": "kFlnw3EFeZxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare 3 models' Precision_recall curves\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(10,5))\n",
        "\n",
        "plot_precision_recall_curve(lr_best, X_test, Y_test, ax=ax,color=\"blue\",label='Logistic Regression')\n",
        "plot_precision_recall_curve(rf_best, X_test, Y_test, ax=ax,color=\"black\",label='Random Forest')\n",
        "plot_precision_recall_curve(xgb_random, X_test, Y_test, ax=ax,color=\"red\",label='XGBoost')\n",
        "\n",
        "plt.title('Precision_Recall of 3 models')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "gWcahd8acEvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dummy classifire for comparision**"
      ],
      "metadata": {
        "id": "posBJCaqemE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dummy classifier to under stand the Black box model\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
        "dummy_clf.fit(X_train, Y_train)\n",
        "DummyClassifier(strategy='stratified')\n",
        "y_pred_dummy = dummy_clf.predict(X_test)\n",
        "\n",
        "print('Dummy model:')\n",
        "get_pre_rec_f1(dummy_clf, X_test, Y_test)"
      ],
      "metadata": {
        "id": "974QAV4YcErc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute precision, recall and threshold of Random Forest\n",
        "\n",
        "Y_predict_rf = rf_best.predict_proba(X_test)\n",
        "Y_scores_rf = Y_predict_rf[:,1]\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(Y_test, Y_scores_rf)\n",
        "\n",
        "recalls_80 = recalls[np.argmin(recalls >= 0.80)]               # Recommend recall score = 0.8\n",
        "precision_80 = precisions[np.argmin(recalls >= 0.80)]\n",
        "threshold_80_recall = thresholds[np.argmin(recalls >= 0.80)]\n",
        "\n",
        "thresholds = np.append(thresholds, 1)\n",
        "\n",
        "recalls_80, precision_80, threshold_80_recall"
      ],
      "metadata": {
        "id": "zDMd3iodlWy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot recommended recall = 0.8\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "\n",
        "ax.plot(thresholds, recalls, label='Recalls')\n",
        "ax.plot(thresholds, precisions, label='Precisions')\n",
        "\n",
        "ax.plot([threshold_80_recall, threshold_80_recall], [precision_80, recalls_80], \"r:\" )\n",
        "ax.plot([threshold_80_recall, threshold_80_recall], [0, precision_80], \"r:\")\n",
        "ax.plot([0, threshold_80_recall], [precision_80, precision_80], \"r:\")\n",
        "ax.plot([0, threshold_80_recall], [recalls_80, recalls_80], \"r:\")\n",
        "ax.plot([threshold_80_recall], [recalls_80], \"ro\", label='Threshold')\n",
        "\n",
        "ax.set_xlabel('Threshold')\n",
        "ax.set_ylabel('Precisons / Recalls')\n",
        "ax.legend(loc='center right',fontsize=16)\n",
        "plt.title('Model Recommendation(Recalls = 0.8)')\n",
        "plt.savefig('recommended_recall')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OkiQcii2lWss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot feature importance of winner model - Random Forest\n",
        "\n",
        "fea_df = pd.DataFrame({'Feature': feature_cols, 'Feature importance': rf_best.feature_importances_})\n",
        "fea_df = fea_df.sort_values(by='Feature importance')\n",
        "\n",
        "figure, ax = plt.subplots(figsize = (10,8))\n",
        "fea_df.plot.barh(x='Feature',y='Feature importance', ax=ax)\n",
        "plt.title('Features importance',fontsize=14)"
      ],
      "metadata": {
        "id": "GTHeHjPClWpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"PAY_1\" AND \"Pay_2\" are the most recent 2 months' payment status and they are the strongest predictors of future payment default risk."
      ],
      "metadata": {
        "id": "hUMuWA7eeKdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression model has the highest recall but the lowest precision, if the business cares recall the most, then this model is the best candidate. If the balance of recall and precision is the most important metric, then Random Forest is the ideal model. Since Random Forest has slightly lower recall but much higher precision than Logistic Regression, I would recommend Random Forest."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}